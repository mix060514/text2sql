
services:
  webui:
    build: .
    restart: always
    ports:
      - "8501:8501"
    volumes:
      - .:/app
      - /app/.venv
    environment:
      - WATCH_FILES=true
      - LLAMA_CPP_API_BASE=http://llama-cpp:8080
      - LLAMA_CPP_VL_API_BASE=http://llama-cpp-vl:8080
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - default
      - ai-stack_default

networks:
  ai-stack_default:
    external: true
